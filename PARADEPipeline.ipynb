{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 42.2M  100 42.2M    0     0  7181k      0  0:00:06  0:00:06 --:--:-- 9226k\n"
     ]
    }
   ],
   "source": [
    "!curl https://zenodo.org/record/3974431/files/vanilla_bert_tiny_on_MSMARCO.tar.gz >> vanilla_bert_tiny_on_MSMARCO.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_models_onMSMARCO/vanilla_bert_tiny_on_MSMARCO/\n",
      "bert_models_onMSMARCO/vanilla_bert_tiny_on_MSMARCO/model.ckpt-1600000.data-00000-of-00001\n",
      "bert_models_onMSMARCO/vanilla_bert_tiny_on_MSMARCO/model.ckpt-1600000.meta\n",
      "bert_models_onMSMARCO/vanilla_bert_tiny_on_MSMARCO/model.ckpt-1600000.index\n"
     ]
    }
   ],
   "source": [
    "!tar -zxvf vanilla_bert_tiny_on_MSMARCO.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 15.7M  100 15.7M    0     0  6607k      0  0:00:02  0:00:02 --:--:-- 6610k\n",
      "Archive:  bert.zip\n",
      "  inflating: bert_model.ckpt.data-00000-of-00001  \n",
      "  inflating: bert_config.json        \n",
      "replace vocab.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: ^C\n"
     ]
    }
   ],
   "source": [
    "!curl https://storage.googleapis.com/bert_models/2020_02_20/uncased_L-2_H-128_A-2.zip >> bert.zip\n",
    "!unzip bert.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyterrier as pt\n",
    "if not pt.started():\n",
    "    pt.init(version=\"snapshot\")\n",
    "    \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "from pyterrier.transformer import TransformerBase\n",
    "\n",
    "from run_reranking import model_fn_builder\n",
    "from input_parser_pyterrier import input_fn_builder\n",
    "from bert.modeling import BertConfig\n",
    "from generate_data import PointwiseInstance\n",
    "from bert import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_instance_pointwise(tokenizer, max_seq_length, qid, docno, query, doc, label):\n",
    "  query = tokenization.convert_to_unicode(query)\n",
    "  doc = tokenization.convert_to_unicode(doc)\n",
    "  passages = get_passages(doc, 150, 50)\n",
    "  if len(passages) == 0:\n",
    "    tf.logging.warn(\"Passage length is 0 in qid {} docno {}\".format(qid, docno))\n",
    "\n",
    "  query = tokenization.convert_to_bert_input(\n",
    "    text=query,\n",
    "    max_seq_length=64,\n",
    "    tokenizer=tokenizer,\n",
    "    add_cls=True,\n",
    "    convert_to_id=False\n",
    "  )\n",
    "  passages = [tokenization.convert_to_bert_input(\n",
    "    text=p,\n",
    "    max_seq_length=max_seq_length-len(query),\n",
    "    tokenizer=tokenizer,\n",
    "    add_cls=False,\n",
    "    convert_to_id=False\n",
    "  ) for p in passages]\n",
    "  instance = PointwiseInstance(\n",
    "    exampleid=\"{}-{}\".format(qid, docno),\n",
    "    tokens_a=query,\n",
    "    tokens_b_list=passages,\n",
    "    relation_label=label\n",
    "  )\n",
    "\n",
    "  return instance\n",
    "\n",
    "def get_passages(text, plen, overlap):\n",
    "    \"\"\" Modified from https://github.com/AdeDZY/SIGIR19-BERT-IR/blob/master/tools/gen_passages.py\n",
    "    :param text:\n",
    "    :param plen:\n",
    "    :param overlap:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    words = text.strip().split(' ')\n",
    "    s, e = 0, 0\n",
    "    passages = []\n",
    "    while s < len(words):\n",
    "      e = s + plen\n",
    "      if e >= len(words):\n",
    "        e = len(words)\n",
    "      # if the last one is shorter than 'overlap', it is already in the previous passage.\n",
    "      if len(passages) > 0 and e - s <= overlap:\n",
    "        break\n",
    "      p = ' '.join(words[s:e])\n",
    "      passages.append(p)\n",
    "      s = s + plen - overlap\n",
    "\n",
    "    if len(passages) > 8:\n",
    "      chosen_ids = sorted(random.sample(range(1, len(passages) - 1), 8 - 2))\n",
    "      chosen_ids = [0] + chosen_ids + [len(passages) - 1]\n",
    "      passages = [passages[id] for id in chosen_ids]\n",
    "\n",
    "    #lobal stats\n",
    "    #stats[len(passages)] += 1\n",
    "    return passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_data import create_int_feature\n",
    "\n",
    "def convert_tokens_to_ids(vocab, tokens):\n",
    "    return [vocab[token] for token in tokens]\n",
    "\n",
    "def write_instance_to_example_files(writer, tokenizer, instance, instance_idx):\n",
    "    \n",
    "    def padding_2d(ids_list, num_tokens_per_segment, padding_value=0):\n",
    "        _len = len(ids_list)\n",
    "        if padding_value == 0:\n",
    "            matrix = np.zeros((_len, num_tokens_per_segment), dtype=np.int)\n",
    "        elif padding_value == 1:\n",
    "            matrix = np.ones((_len, num_tokens_per_segment), dtype=np.int)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupport padding value\")\n",
    "\n",
    "        for i, _list in enumerate(ids_list):\n",
    "            matrix[i, :len(_list)] = _list\n",
    "\n",
    "        return matrix.flatten()\n",
    "\n",
    "    tokens_a = instance.tokens_a\n",
    "    tokens_b_list = instance.tokens_b_list\n",
    "    tokens_a_ids = convert_tokens_to_ids(tokenizer.vocab, tokens_a)\n",
    "    tokens_b_list = [convert_tokens_to_ids(tokenizer.vocab, p) for p in tokens_b_list]\n",
    "    label = instance.relation_label\n",
    "    assert len(tokens_b_list) <= 8\n",
    "    num_segments = len(tokens_b_list)\n",
    "\n",
    "    input_ids = [tokens_a_ids + tokens_b_passage_ids for tokens_b_passage_ids in tokens_b_list]\n",
    "    tokens_a_len = len(tokens_a_ids)  # helpful for segment ids\n",
    "    input_ids_lens = [len(input_id) for input_id in input_ids]  # helpful for input mask\n",
    "    input_ids_lens = input_ids_lens + [128] * (8 - len(input_ids_lens))\n",
    "    input_ids = padding_2d(input_ids, 128, padding_value=0)\n",
    "    # write to tfrecord\n",
    "    features = collections.OrderedDict()\n",
    "    features[\"input_ids\"] = create_int_feature(input_ids)\n",
    "    features[\"tokens_a_len\"] = create_int_feature([tokens_a_len])\n",
    "    features[\"tokens_ids_lens\"] = create_int_feature(input_ids_lens)\n",
    "    features[\"num_segments\"] = create_int_feature([num_segments])\n",
    "    features[\"label\"] = create_int_feature([label])\n",
    "    tf_example = tf.train.Example(features=tf.train.Features(feature=features))\n",
    "    writer.write(tf_example.SerializeToString())\n",
    "\n",
    "    if instance_idx < 5:\n",
    "        tf.logging.info(\"*** Example ***\")\n",
    "        tf.logging.info(\"tokens_a: %s\" % \" \".join(\n",
    "            [tokenization.printable_text(x) for x in instance.tokens_a]))\n",
    "        tf.logging.info(\"tokens_b_list: {}\".format(instance.tokens_b_list))\n",
    "\n",
    "        for feature_name in features.keys():\n",
    "            feature = features[feature_name]\n",
    "            values = []\n",
    "            if feature.int64_list.value:\n",
    "                values = feature.int64_list.value\n",
    "            elif feature.float_list.value:\n",
    "                values = feature.float_list.value\n",
    "            tf.logging.info(\n",
    "                \"%s: %s\" % (feature_name, \" \".join([str(x) for x in values])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_to_feature_list(df, tokenizer, writer):\n",
    "    feature_list = []\n",
    "    for ind, line in enumerate(df.itertuples()):\n",
    "        instance = create_instance_pointwise(\n",
    "                            tokenizer=tokenizer,\n",
    "                            max_seq_length=512,\n",
    "                            qid=line.qid,\n",
    "                            docno=line.docno,\n",
    "                            query=line.query,\n",
    "                            doc=line.body,\n",
    "                            label=0\n",
    "        )\n",
    "        \n",
    "        write_instance_to_example_files(writer, tokenizer, instance, ind)\n",
    "\n",
    "        \n",
    "class PARADEPipeline(TransformerBase):\n",
    "    def __init__(self, aggregation_method):\n",
    "        self.aggregation_method = aggregation_method #'cls_max',  'cls_avg', 'cls_attn' or 'cls_transformer'\n",
    "        self.tokenizer = tokenization.FullTokenizer(vocab_file='vocab.txt')\n",
    "        self.writer = tf.python_io.TFRecordWriter(\"output.tfrecords\")\n",
    "        self.run_config = tf.estimator.tpu.RunConfig(\n",
    "            cluster=None,\n",
    "            model_dir=None,\n",
    "            save_checkpoints_steps=1000,\n",
    "            keep_checkpoint_max=1,\n",
    "            tpu_config=None)\n",
    "            \n",
    "        '''\n",
    "        tpu_config is set to None since we aren't using a tpu, but in case we actually need this\n",
    "        i'll just keep this commented out instead of deleting it\n",
    "            \n",
    "            tpu_config=tf.estimator.tpu.TPUConfig(\n",
    "                iterations_per_loop=1000,\n",
    "                num_shards=8,\n",
    "                per_host_input_for_training=is_per_host))\n",
    "        '''\n",
    "        \n",
    "        self.model_fn = model_fn_builder(\n",
    "            bert_config=BertConfig.from_json_file('bert_models_onMSMARCO/vanilla_bert_tiny_on_MSMARCO/bert_config.json'),\n",
    "            num_labels=2,\n",
    "            init_checkpoint='bert_models_onMSMARCO/vanilla_bert_tiny_on_MSMARCO/model.ckpt-1600000',\n",
    "            learning_rate=5e-5,\n",
    "            num_train_steps=None,\n",
    "            num_warmup_steps=None,\n",
    "            use_tpu=False,\n",
    "            use_one_hot_embeddings=False,\n",
    "            aggregation_method=self.aggregation_method,\n",
    "            pretrained_model='bert',\n",
    "            from_distilled_student=False)\n",
    "        \n",
    "        self.estimator = tf.estimator.tpu.TPUEstimator(\n",
    "            use_tpu=False,\n",
    "            model_fn=self.model_fn,\n",
    "            config=self.run_config,\n",
    "            train_batch_size=32,\n",
    "            eval_batch_size=32,\n",
    "            predict_batch_size=32)\n",
    "        \n",
    "        \n",
    "    def transform(self, queries_and_docs):\n",
    "        df_to_feature_list(queries_and_docs, self.tokenizer, self.writer)\n",
    "        \n",
    "        eval_input_fn = input_fn_builder(\n",
    "            dataset_path=\"output.tfrecords\",\n",
    "            max_num_segments_perdoc=8,\n",
    "            max_seq_length=128,\n",
    "            is_training=False)\n",
    "            \n",
    "        result = self.estimator.predict(input_fn=eval_input_fn, yield_single_examples=True)\n",
    "        \n",
    "        results = []\n",
    "        for item in result:\n",
    "            pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>query</th>\n",
       "      <th>docno</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>q1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>doc1</td>\n",
       "      <td>professor proton demonstrated the chemical rea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>q1</td>\n",
       "      <td>chemical reactions</td>\n",
       "      <td>doc2</td>\n",
       "      <td>chemical brothers is great techno music</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  qid               query docno  \\\n",
       "0  q1  chemical reactions  doc1   \n",
       "1  q1  chemical reactions  doc2   \n",
       "\n",
       "                                                body  \n",
       "0  professor proton demonstrated the chemical rea...  \n",
       "1            chemical brothers is great techno music  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "q = \"chemical reactions\"\n",
    "doc1 = \"professor proton demonstrated the chemical reaction\"\n",
    "doc2 = \"chemical brothers is great techno music\"\n",
    "\n",
    "df = pd.DataFrame([[\"q1\", q, \"doc1\", doc1], [\"q1\", q, \"doc2\", doc2]], columns=[\"qid\", \"query\", \"docno\", \"body\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Estimator's model_fn (<function model_fn_builder.<locals>.model_fn at 0x7f7bbb1d5e18>) includes params argument, but params are not passed to Estimator.\n",
      "WARNING:tensorflow:Using temporary folder as model directory: /tmp/tmpcdqnjy2b\n",
      "INFO:tensorflow:Using config: {'_model_dir': '/tmp/tmpcdqnjy2b', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': 1000, '_save_checkpoints_secs': None, '_session_config': allow_soft_placement: true\n",
      "graph_options {\n",
      "  rewrite_options {\n",
      "    meta_optimizer_iterations: ONE\n",
      "  }\n",
      "}\n",
      ", '_keep_checkpoint_max': 1, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': None, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_service': None, '_cluster_spec': <tensorflow.python.training.server_lib.ClusterSpec object at 0x7f7bb9bf1400>, '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1, '_tpu_config': TPUConfig(iterations_per_loop=2, num_shards=None, num_cores_per_replica=None, per_host_input_for_training=2, tpu_job_name=None, initial_infeed_sleep_secs=None, input_partition_dims=None, eval_training_input_configuration=2, experimental_host_call_every_n_steps=1), '_cluster': None}\n",
      "INFO:tensorflow:_TPUContext: eval_on_tpu True\n",
      "WARNING:tensorflow:eval_on_tpu ignored because use_tpu is False.\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens_a: [CLS] chemical reactions [SEP]\n",
      "INFO:tensorflow:tokens_b_list: [['professor', 'proton', 'demonstrated', 'the', 'chemical', 'reaction', '[SEP]']]\n",
      "INFO:tensorflow:input_ids: 101 5072 9597 102 2934 20843 7645 1996 5072 4668 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:tokens_a_len: 4\n",
      "INFO:tensorflow:tokens_ids_lens: 11 128 128 128 128 128 128 128\n",
      "INFO:tensorflow:num_segments: 1\n",
      "INFO:tensorflow:label: 0\n",
      "INFO:tensorflow:*** Example ***\n",
      "INFO:tensorflow:tokens_a: [CLS] chemical reactions [SEP]\n",
      "INFO:tensorflow:tokens_b_list: [['chemical', 'brothers', 'is', 'great', 'techno', 'music', '[SEP]']]\n",
      "INFO:tensorflow:input_ids: 101 5072 9597 102 5072 3428 2003 2307 21416 2189 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "INFO:tensorflow:tokens_a_len: 4\n",
      "INFO:tensorflow:tokens_ids_lens: 11 128 128 128 128 128 128 128\n",
      "INFO:tensorflow:num_segments: 1\n",
      "INFO:tensorflow:label: 0\n",
      "INFO:tensorflow:Could not find trained model in model_dir: /tmp/tmpcdqnjy2b, running initialization to predict.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Running infer on CPU\n",
      "INFO:tensorflow:*** Features ***\n",
      "INFO:tensorflow:  name = input_ids, shape = (32, 8, 128)\n",
      "INFO:tensorflow:  name = input_mask, shape = (32, 8, 128)\n",
      "INFO:tensorflow:  name = label, shape = (32,)\n",
      "INFO:tensorflow:  name = num_segments, shape = (32,)\n",
      "INFO:tensorflow:  name = segment_ids, shape = (32, 8, 128)\n",
      "WARNING:tensorflow:From /tf/PARADE/bert/modeling.py:707: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "INFO:tensorflow:**** Trainable Variables ****\n",
      "INFO:tensorflow:  name = bert/embeddings/word_embeddings:0, shape = (30522, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/token_type_embeddings:0, shape = (2, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/position_embeddings:0, shape = (512, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/beta:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/embeddings/LayerNorm/gamma:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/kernel:0, shape = (128, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/query/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/kernel:0, shape = (128, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/key/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/kernel:0, shape = (128, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/self/value/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/kernel:0, shape = (128, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/dense/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/beta:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/attention/output/LayerNorm/gamma:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/kernel:0, shape = (128, 512), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/intermediate/dense/bias:0, shape = (512,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/kernel:0, shape = (512, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/dense/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/beta:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_0/output/LayerNorm/gamma:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/kernel:0, shape = (128, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/query/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/kernel:0, shape = (128, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/key/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/kernel:0, shape = (128, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/self/value/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/kernel:0, shape = (128, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/dense/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/beta:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/attention/output/LayerNorm/gamma:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/kernel:0, shape = (128, 512), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/intermediate/dense/bias:0, shape = (512,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/kernel:0, shape = (512, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/dense/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/beta:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/encoder/layer_1/output/LayerNorm/gamma:0, shape = (128,), *INIT_FROM_CKPT*\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:  name = bert/pooler/dense/kernel:0, shape = (128, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = bert/pooler/dense/bias:0, shape = (128,), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_weights:0, shape = (2, 128), *INIT_FROM_CKPT*\n",
      "INFO:tensorflow:  name = output_bias:0, shape = (2,), *INIT_FROM_CKPT*\n",
      "WARNING:tensorflow:From /tf/PARADE/run_reranking.py:310: The name tf.estimator.tpu.TPUEstimatorSpec is deprecated. Please use tf.compat.v1.estimator.tpu.TPUEstimatorSpec instead.\n",
      "\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/array_ops.py:1475: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n",
      "INFO:tensorflow:prediction_loop marked as finished\n",
      "INFO:tensorflow:prediction_loop marked as finished\n"
     ]
    }
   ],
   "source": [
    "pipeline = PARADEPipeline(aggregation_method='cls_max')\n",
    "\n",
    "#vaswani  = pt.datasets.get_dataset(\"vaswani\")\n",
    "#vaswani.get_corpus()\n",
    "\n",
    "\n",
    "pipeline(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
